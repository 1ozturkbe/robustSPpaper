\section{Introduction}
Robust optimization methods provide tractable methods to capture uncertainty in design.

TODO: Motivate the use of robust opt. for aircraft design over traditional methods, and stochastic/UQ.

Geometric programming is a method of log-convex optimization for which robust formulations exist.
However, the stringent mathematical requirements of a \gls{gp} limits its application to non-log-convex problems.
The \gls{sp} is the difference-of-log-convex extension of the \gls{gp} which can be applied to
solve this larger set of problems, albeit with the loss of some mathematical guarantees compared to the \gls{gp}.
In this paper, we propose a tractable \gls{rsp} which we solve as a sequential \gls{rgp},
allowing us to implement robustness in non-log-convex problems.

We implement the \gls{rsp} formulation on a simple aircraft design problem with 19 free variables,
12 uncertain parameters and 17 constraints to demonstrate its potential. We believe that aircraft design
problems can especially benefit from robustness. Oftentimes, aerospace engineers will implement
margins in the design process to account for uncertainties in parameters that a design may be sensitive to,
without explicit knowledge of the trade-off between robustness and optimality~\cite{yao2011}.

TODO: More depth/references as to methods for UQ/RO in aircraft design.

A robust aircraft design formulation will allow designers to allocate margin more effectively
to obtain better-performing designs with feasibility guarantees.

\subsection{Geometric Programs}

Geometric programming is a method for log-convex optimization.
\gls{gp}s offer globally optimal solutions with no initial guesses,
and allow engineers and designers to solve large-scale ($>$10,000 variables) nonlinear problems
on the order of $\sim1$ second, given some stringent mathematical requirements.
Given the general optimization problem below:

\begin{align}
\label{e:gpform}
\textrm{minimize } f_0(\mathbf{x}) & \nonumber \\
\textrm{subject to } f_i(\mathbf{x}) &\leq 1, i=1,...,m \\
g_i (\mathbf{x}) &= 1, i = 1,...,p \nonumber
\end{align}

In a \gls{gp}, $f_{i}$s and $g_{i}$s must have special posynomial and monomial forms below, with strictly positive coefficients:

A monomial is a function of the form
\begin{equation}\label{e:monomial}
g(\mathbf{x}) = e^{\mathbf{a}\mathbf{x} + b}
\end{equation}
where $\mathbf{a} \in \mathbb{R}^n, b \in \mathbb{R}$ and $\mathbf{x} \in \mathbb{R}^n$.

A posynomial is a function of the form
\begin{equation}\label{e:posynomial}
f(\mathbf{x}) = \sum_{k=1}^{K}e^{\mathbf{a}_k\mathbf{x} + b_k}
\end{equation}
where $\vec{a}_k \in \mathbb{R}^n, b_k \in \mathbb{R}$ and $\vec{x} \in \mathbb{R}^n$.
A posynomial is a sum of monomials. Therefore, all monomials are also one-term posynomials.

Any monomial constraint $g(\vec{x}) = 1$ can be represented as $g(\vec{x}) \leq 1$ and $\frac{1}{g(\vec{x})} \leq 1$,
therefore, the standard \gls{gp} can be written in the inequality constrained form as follows:

\begin{equation}
\begin{aligned}
& \text{minimize} && \textstyle{\sum}_{k=1}^{K_0}e^{\vec{a}_{0k}\vec{x} + b_{0k}} \\
& \text{subject to} && \textstyle{\sum}_{k=1}^{K_i}e^{\vec{a}_{ik}\vec{x} + b_{ik}} \leq 1 \quad \forall i \in 1,...,m\\
\end{aligned}
\label{GP_inequality}
\end{equation}

A tractable approximation of the \gls{rgp} exists and will be discussed, and has been formulated with polyhedral
and ellipsoidal uncertainty sets. \gls{rgp}s allow for uncertainty in both the coefficients $b_k$ and the exponents $\mathbf{a}_{k}$.
However, the restriction on the sign of the coefficients limits \gls{gp}s (and consequently \gls{rgp}s) to certain classes of problems.
Signomials are constraints of the same form as $f(x)$, but allow for negative coefficients in $f(x)$.
To be able to find robust solutions to \gls{sp}s, we would like to create a framework to generalize the
uncertainty sets used in \gls{rgp}s to \gls{rsp}s.


\subsection{Signomial Programs}

Signomials allow us to solve non-log-convex problems as sequential geometric programs.
Signomials are a difference of posynomials shown in Equation~\ref{e:posynomial}.
The log transform of an SP is not a convex optimization problem, but instead a
difference of convex optimization problem that can be written in log-space as

\begin{equation}
\begin{aligned}
\text{minimize }f_{0}(\mathbf{x})& \\
\text{subject to }f_{i}(\mathbf{x}) - h_{i}(\mathbf{x})& \leq 0, i = 1, ...., m \\
\end{aligned}
\end{equation}

where $f_{i}$ and $h_{i}$ are posynomials. This problem can be reliably solved
iteratively by taking the monomial approximation of $g_{i}$ with a solution or
initial guess $x_{i}$, solving this \gls{gp} approximation to obtain a new solution $x_{i+1}$,
and repeating the process until some convergence parameter is satisfied in the objective function.

The SP algorithm is well-studied, reliably solving SPs with an initial guess of all 1â€™s.
SPs are guaranteed to be sub-optimal but feasible solutions to a general non-linear problem.

