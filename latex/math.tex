\section{Mathematical Background}

\subsection{Robust Optimization}

Given a general optimization problem under parametric uncertainty, we define the set of possible
realizations of uncertain vector of parameters $u$ in the uncertainty set $\mathcal{U}$. This
allows us to define the problem under uncertainty below.
\begin{align*}
    \text{min} &~f_0(x) \\
    \text{s.t.}     &~f_i(x,u) \leq 0,~\forall u \in \mathcal{U},~i = 1,\ldots,n
\end{align*}
This problem is infinite-dimensional, since it is possible to formulate an infinite number of constraints
with the countably infinite number of possible realizations of $u \in \mathcal{U}$. To circumvent this issue,
we can define the following robust formulation of the uncertain problem below.
\begin{align*}
    \text{min} &~f_0(x) \\
    \text{s.t.}     &~\underset{u \in \mathcal{U}}{\text{max}}~f_i(x,u) \leq 0,~i = 1,\ldots,n
\end{align*}
This formulation hedges against the worst-case realization of the uncertainty in the defined uncertainty
set. The set is often described by a norm, which contains possible uncertain outcomes from distributions with
bounded support
\begin{equation}
\begin{aligned}
    \text{min} &~f_0(x) \\
    \text{s.t.}     &~\underset{u}{\text{max}}~f_i(x,u) \leq 0,~i = 1,\ldots,n
                    &~\norm{u} \leq \Gamma \\
\end{aligned}
        \label{eq:normform}
\end{equation}
where $\Gamma$ is defined by the user as a global uncertainty bound. The larger the $\Gamma$,
the greater the size of the uncertainty set that is protected against.

\subsection{Geometric Programming}

A \emph{geometric program in posynomial form} is a log-convex optimization problem of the form:
\begin{equation}
\begin{aligned}
	& \text{min} && f_0 \left(\vec{u}\right) \\
	& \text{s.t.} && f_i \left(\vec{u}\right) \leq 1, i = 1,...,m_p\\
	& && h_i \left(\vec{u}\right) = 1, i = 1, ...,m_e
\end{aligned}
\label{GP_standard}
\end{equation}
where each $f_i$ is a {\em posynomial}, each $h_i$ is a {\em monomial}, $m_p$ is the number of posynomials,
and $m_e$ is the number of monomials. A monomial $h(\vec{u})$ is a function of the form:
\begin{equation}
	h_i(\vec{u}) = e^{b_i}\textstyle{\prod}_{j=1}^{n}{u_j}^{a_{ij}}
\end{equation}
where $a_{ij}$ is the $j^{th}$ component of a row vector $\vec{a_i}$ in $\mathbb{R}^n$,
$u_j$ is the $j^{th}$ component of a column vector $\vec{u}$ in $\mathbb{R}^n_+$ ,
and $b_i$ is in $\mathbb{R}$. An example of a monomial is the lift equation,
$L = \frac{1}{2}\rho V^2 C_L S$. A posynomial $f(\vec{u})$ is the sum of $K \in \mathbb{Z}^+$ monomials:
\begin{equation}
	f_i(\vec{u}) = \textstyle{\sum_{k=1}^{K}}e^{b_{ikj}}\prod_{j=1}^{n}{u_j}^{a_{ikj}}
\end{equation}
where $a_{ikj}$ is the $j^{th}$ component of a row vector $\vec{a_{ik}}$ in $\mathbb{R}^n$,
$u_j$ is the $j^{th}$ component of a column vector $\vec{u}$ in $\mathbb{R}^n_+$, and $b_{ik}$
is in $\mathbb{R}$ \cite{Boyd2007}. The stagnation pressure definition is a good example:
$P_t = P + \frac{1}{2} \rho V^2$.\\

A logarithmic change of the variables $x_j = \log(u_j)$ would turn a monomial into
{\em  the exponential of an affine function} and a posynomial into
{\em the sum of exponentials of affine functions}. A transformed monomial $h_i(\vec{x})$ is of the form:
\begin{equation}
    h_i(\vec{x}) = e^{\vec{a_i}\vec{x} + b_i}
\end{equation}
where $\vec{x}$ is a column vector in $\mathbb{R}^n$.
A transformed posynomial $f_i(\vec{x})$ is the sum of $K_i \in \mathbb{Z}^+$ monomials,
\begin{equation}
    f_i(\vec{x}) = \textstyle{\sum_{k=1}^{K_i}}e^{\vec{a_{ik}}\vec{x} + b_{ik}}
\end{equation}
where $\vec{x}$ is a column vector in $\mathbb{R}^n$.
A geometric program with transformed constraints is a \emph{geometric program in exponential form}, and
is a convex optimization problem.

The positivity of exponential functions restricts the space spanned by posynomials and limits
\gls{gp}s to certain classes of problems.
However, since many engineering problems of interest have purely positive quantities \gls{gp}s
are quite applicable, and certain variable transformations can make problems with negative quantities tractable.
The restriction of posynomials to the \emph{less-than-side of
inequalities} is a more significant barrier, and motivates the introduction of signomials.

\subsection{Signomial Programming}
A {\em signomial} can be defined as the difference between two posynomials. Consequently,
a \gls{sp} is a non-log-convex optimization problem of the form:
\begin{equation}
\begin{aligned}
&\text{minimize } && f_{0}(\vec{x}) \\
&\text{subject to } && f_{i}(\vec{x}) - g_{i}(\vec{x})& \leq 0, i = 1, ...., m \\
\end{aligned}
\end{equation}
where $f_{i}$ and $g_{i}$ are both posynomials, and $\vec{x}$ is a column vector in $\mathbb{R}^n$. 

Reliably solving a \gls{sp} to a local optimum has been described in \cite{Boyd2007} and \cite{Lipp2016}.
A common solution heuristic involves solving a \gls{sp} as a sequence of \gls{gp}s,
where each \gls{gp} is a local approximation of the \gls{sp}.
Although it is a powerful tool, applications involving \gls{sp}s are usually prone
to uncertainties that have a significant effect on the solution.
